# ğŸš€ Data Pipelines with Apache Airflow

## ğŸ“Œ Project Overview
This project builds an **ETL pipeline** for Sparkify, a music streaming service, using **Apache Airflow** and **Amazon Redshift**. It automates data ingestion, transformation, and validation for structured analytics.

### **ğŸŒŸ Features**
âœ… **Airflow DAGs** to orchestrate data pipeline  
âœ… **Redshift Warehouse** for scalable analytics  
âœ… **Custom Airflow Operators** for ETL tasks  
âœ… **Data Quality Checks** to ensure integrity  
âœ… **Dynamic and Reusable** pipeline  

---

## ğŸ’½ Project Structure
```
data-pipelines-airflow/
â”‚â”€â”€ dags/                  # Airflow DAGs
â”‚   â”œâ”€â”€ create_tables_dag.py
â”‚   â”œâ”€â”€ etl_dag.py
â”‚
â”‚â”€â”€ plugins/               # Custom Airflow Operators
â”‚   â”œâ”€â”€ operators/         
â”‚   â”‚   â”œâ”€â”€ stage_redshift.py
â”‚   â”‚   â”œâ”€â”€ load_fact.py
â”‚   â”‚   â”œâ”€â”€ load_dimension.py
â”‚   â”‚   â”œâ”€â”€ data_quality.py
â”‚   â”œâ”€â”€ helpers/           # SQL Query Helper
â”‚   â”‚   â”œâ”€â”€ sql_queries.py
â”‚
â”‚â”€â”€ sql/                   # SQL scripts
â”‚   â”œâ”€â”€ create_tables.sql
â”‚
â”‚â”€â”€ docs/                  # Documentation (DAG Images)
â”‚   â”œâ”€â”€ create_tables_dag.png
â”‚   â”œâ”€â”€ etl_dag.png
â”‚
â”‚â”€â”€ logs/                  # Airflow logs (ignored in .gitignore)
â”‚â”€â”€ .env                   # Environment variables (ignored in .gitignore)
â”‚â”€â”€ README.md
```

---

## ğŸ› ï¸ DAGs & Workflow
### **1ï¸âƒ£ Create Tables DAG**
This DAG initializes the Redshift database by creating required tables.

![Create Tables DAG](docs/create_tables_dag.png)

### **2ï¸âƒ£ ETL DAG**
This DAG performs the ETL process:
1. **Extract** logs & songs data from S3  
2. **Transform** raw data into structured analytics tables  
3. **Load** into Redshift for querying  

![ETL DAG](docs/etl_dag.png)

---

## ğŸš€ **Technologies Used**
- ğŸ  **Apache Airflow** - Orchestration  
- â˜ï¸ **Amazon S3** - Data Storage  
- ğŸ”´ **Amazon Redshift** - Data Warehouse  
- ğŸ– **Python** - ETL Scripts  
- ğŸ” **SQL** - Data Transformation  

---

## âš™ï¸ **Setup Instructions**
### **1ï¸âƒ£ Clone the Repository**
```sh
git clone https://github.com/Shubhistaken/data-pipelines-airflow.git
cd data-pipelines-airflow
```

### **2ï¸âƒ£ Install Airflow & Dependencies**
```sh
pip install apache-airflow apache-airflow-providers-amazon 
```

### **3ï¸âƒ£ Configure Airflow Connections**
1. **Go to Airflow UI â†’ Admin â†’ Connections**
2. Create a connection named **`redshift`**:
   - Conn Type: `Postgres`
   - Host: `<your-redshift-cluster-endpoint>`
   - Schema: `dev`
   - Login: `<your-db-user>`
   - Password: `<your-db-password>`
   - Port: `5439`
3. Create a connection named **`aws_credentials`**:
   - Conn Type: `Amazon Web Services`
   - Login: `Your AWS Access Key`
   - Password: `Your AWS Secret Key`

### **4ï¸âƒ£ Set Airflow Variables**
Go to **Airflow UI â†’ Admin â†’ Variables**, and add:
| Key                     | Value |
|-------------------------|------------------------------|
| `s3_bucket`            | `shubh-airflow-project` |
| `s3_prefix_log_data`   | `log-data/` |
| `s3_prefix_song_data`  | `song-data/` |
| `s3_prefix_log_json_path` | `log-json-path.json` |
| `region`               | `us-west-2` |

### **5ï¸âƒ£ Start Airflow**
```sh
airflow scheduler & airflow webserver
```

### **6ï¸âƒ£ Run the DAGs**
1. Trigger `create_tables_dag` first
2. Trigger `etl_dag` next

---

## ğŸ‘‡ **Operators & Functionality**
| Operator | Description |
|----------|-------------|
| `StageToRedshiftOperator` | Copies raw JSON data from S3 to Redshift staging tables |
| `LoadFactOperator` | Loads data into the fact table (`songplays`) |
| `LoadDimensionOperator` | Loads data into dimension tables (`songs`, `users`, `artists`, `time`) |
| `DataQualityOperator` | Ensures row counts and NULL checks |

---

## ğŸ“„ **Data Model (Star Schema)**
```sql
-- Fact Table
songplays(songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)

-- Dimension Tables
users(user_id, first_name, last_name, gender, level)
songs(song_id, title, artist_id, year, duration)
artists(artist_id, name, location, latitude, longitude)
time(start_time, hour, day, week, month, year, weekday)
```

---

## ğŸ“š **License**
This project is for educational purposes only.

---

## â­ **Contributions**
Feel free to submit issues or pull requests.

---

### **ğŸ”— References**
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Amazon Redshift Documentation](https://docs.aws.amazon.com/redshift/latest/dg/welcome.html)
