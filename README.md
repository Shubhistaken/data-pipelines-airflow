# ğŸš€ Data Pipelines with Apache Airflow

## ğŸ“Œ Project Overview
This project builds an **ETL pipeline** for Sparkify, a music streaming service, using **Apache Airflow** and **Amazon Redshift**. It automates data ingestion, transformation, and validation for structured analytics.

### **ğŸŒŸ Features**
âœ… **Airflow DAGs** to orchestrate data pipeline  
âœ… **Redshift Warehouse** for scalable analytics  
âœ… **Custom Airflow Operators** for ETL tasks  
âœ… **Data Quality Checks** to ensure integrity  
âœ… **Dynamic and Reusable** pipeline  

---

## ğŸ“ Project Structure




## ğŸ› ï¸ **DAGs & Workflow**
### **1ï¸âƒ£ Create Tables DAG**
This DAG initializes the Redshift database by creating required tables.

![Create Tables DAG](docs/create_tables_dag.png)

### **2ï¸âƒ£ ETL DAG**
This DAG performs the ETL process:
1. **Extract** logs & songs data from S3  
2. **Transform** raw data into structured analytics tables  
3. **Load** into Redshift for querying  

![ETL DAG](docs/etl_dag.png)

---

## ğŸš€ **Technologies Used**
- ğŸ— **Apache Airflow** - Orchestration  
- â˜ï¸ **Amazon S3** - Data Storage  
- ğŸ”´ **Amazon Redshift** - Data Warehouse  
- ğŸ **Python** - ETL Scripts  
- ğŸ” **SQL** - Data Transformation

- 
